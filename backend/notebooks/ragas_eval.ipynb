{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation\n",
    "\n",
    "#### Choose an advanced retrieval technique that you believe will improve your application’s ability to retrieve the most appropriate context. Write 1-2 sentences on why you believe it will be useful for your use case.\n",
    "\n",
    "This evaluation compares the performance of **Hybrid (RRF only)** vs **Hybrid + Cohere Reranking**.\n",
    "\n",
    "Hybrid with RRF represents the baseline retrieval strategy that I would consider useful for my use case, since I need both keyword and semantic search to meet my user's expectations. I want to measure the baseline performance of that strategy, then I would like to measure the performance with Cohere reranking added. I believe that reranking is useful for my use case because it is an additional layer of refinement on results that can help retrieval precision. Precise retrieval is extremely important to my users,  surfacing the right asset quickly is their primary goal. The result of this assessment will help me determine whether to add reranking to my final application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI key loaded: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "print(\"OpenAI key loaded:\", bool(os.getenv(\"OPENAI_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 test cases\n"
     ]
    }
   ],
   "source": [
    "# Test dataset - queries with ground truth answers\n",
    "EVAL_DATASET = [\n",
    "    {\n",
    "        \"question\": \"sunset over water\",\n",
    "        \"ground_truth\": \"Images showing sunsets with water, ocean, lake, or sea. Warm orange and pink colors in the sky reflected on water.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"portrait of woman\",\n",
    "        \"ground_truth\": \"Portrait photographs of women, headshots or upper body, with focus on the face.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"running dog\",\n",
    "        \"ground_truth\": \"Action shots of dogs running, playing, or in motion outdoors.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"bowl of fruit\",\n",
    "        \"ground_truth\": \"Still life images of fruit in bowls or arranged on tables.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"cozy autumn\",\n",
    "        \"ground_truth\": \"Fall imagery with warm colors - oranges, reds, browns. Cozy atmosphere, leaves, sweaters, warm drinks.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"strong contrast\",\n",
    "        \"ground_truth\": \"High contrast images with dramatic lighting, deep shadows, bright highlights. Black and white or bold tonal range.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(EVAL_DATASET)} test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API connected: {'status': 'ok'}\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import time\n",
    "\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "def search(query: str, mode: str = \"hybrid\", limit: int = 5, rerank: bool = True) -> list[dict]:\n",
    "    \"\"\"Call Picosearch API.\"\"\"\n",
    "    resp = httpx.post(\n",
    "        f\"{API_URL}/search\",\n",
    "        json={\"query\": query, \"mode\": mode, \"limit\": limit, \"rerank\": rerank},\n",
    "        timeout=60.0\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"results\"]\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    health = httpx.get(f\"{API_URL}/health\").json()\n",
    "    print(\"API connected:\", health)\n",
    "except Exception as e:\n",
    "    print(f\"API not running: {e}\")\n",
    "    print(\"Start the backend: cd backend && uv run uvicorn main:app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "ANSWER_PROMPT = \"\"\"Based on the following image descriptions from a search, answer the user's query.\n",
    "Describe what images were found and how well they match the query.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Retrieved image descriptions:\n",
    "{contexts}\n",
    "\n",
    "Answer (2-3 sentences):\"\"\"\n",
    "\n",
    "def generate_answer(query: str, contexts: list[str]) -> str:\n",
    "    \"\"\"Generate answer from retrieved contexts.\"\"\"\n",
    "    context_str = \"\\n\".join(f\"- {c}\" for c in contexts)\n",
    "    prompt = ANSWER_PROMPT.format(query=query, contexts=context_str)\n",
    "    return llm.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: sunset over water\n",
      "Processing: portrait of woman\n",
      "Processing: running dog\n",
      "Processing: bowl of fruit\n",
      "Processing: cozy autumn\n",
      "Processing: strong contrast\n",
      "\n",
      "Collected 6 baseline results\n"
     ]
    }
   ],
   "source": [
    "# BASELINE: Hybrid WITHOUT reranking\n",
    "\n",
    "baseline_results = []\n",
    "\n",
    "for row in EVAL_DATASET:\n",
    "    print(f\"Processing: {row['question']}\")\n",
    "\n",
    "    results = search(row[\"question\"], mode=\"hybrid\", limit=5, rerank=False)\n",
    "    contexts = [r[\"description\"] for r in results if r.get(\"description\")]\n",
    "    answer = generate_answer(row[\"question\"], contexts) if contexts else \"No results found.\"\n",
    "\n",
    "    baseline_results.append({\n",
    "        \"user_input\": row[\"question\"],\n",
    "        \"retrieved_contexts\": contexts,\n",
    "        \"response\": answer,\n",
    "        \"reference\": row[\"ground_truth\"],\n",
    "    })\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\nCollected {len(baseline_results)} baseline results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: sunset over water\n",
      "Processing: portrait of woman\n",
      "Processing: running dog\n",
      "Processing: bowl of fruit\n",
      "Processing: cozy autumn\n",
      "Processing: strong contrast\n",
      "\n",
      "Collected 6 reranked results\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED: Hybrid WITH Cohere reranking\n",
    "\n",
    "rerank_results = []\n",
    "\n",
    "for row in EVAL_DATASET:\n",
    "    print(f\"Processing: {row['question']}\")\n",
    "\n",
    "    results = search(row[\"question\"], mode=\"hybrid\", limit=5, rerank=True)\n",
    "    contexts = [r[\"description\"] for r in results if r.get(\"description\")]\n",
    "    answer = generate_answer(row[\"question\"], contexts) if contexts else \"No results found.\"\n",
    "\n",
    "    rerank_results.append({\n",
    "        \"user_input\": row[\"question\"],\n",
    "        \"retrieved_contexts\": contexts,\n",
    "        \"response\": answer,\n",
    "        \"reference\": row[\"ground_truth\"],\n",
    "    })\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\nCollected {len(rerank_results)} reranked results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vv/l_xmncsj59gbr7ptc7h1ghvm0000gn/T/ipykernel_15703/360945677.py:2: DeprecationWarning: Importing LLMContextRecall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import LLMContextRecall\n",
      "  from ragas.metrics import LLMContextRecall, Faithfulness, ResponseRelevancy, LLMContextPrecisionWithoutReference\n",
      "/var/folders/vv/l_xmncsj59gbr7ptc7h1ghvm0000gn/T/ipykernel_15703/360945677.py:2: DeprecationWarning: Importing Faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import Faithfulness\n",
      "  from ragas.metrics import LLMContextRecall, Faithfulness, ResponseRelevancy, LLMContextPrecisionWithoutReference\n",
      "/var/folders/vv/l_xmncsj59gbr7ptc7h1ghvm0000gn/T/ipykernel_15703/360945677.py:2: DeprecationWarning: Importing ResponseRelevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import ResponseRelevancy\n",
      "  from ragas.metrics import LLMContextRecall, Faithfulness, ResponseRelevancy, LLMContextPrecisionWithoutReference\n",
      "/var/folders/vv/l_xmncsj59gbr7ptc7h1ghvm0000gn/T/ipykernel_15703/360945677.py:2: DeprecationWarning: Importing LLMContextPrecisionWithoutReference from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import LLMContextPrecisionWithoutReference\n",
      "  from ragas.metrics import LLMContextRecall, Faithfulness, ResponseRelevancy, LLMContextPrecisionWithoutReference\n",
      "/var/folders/vv/l_xmncsj59gbr7ptc7h1ghvm0000gn/T/ipykernel_15703/360945677.py:5: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n"
     ]
    }
   ],
   "source": [
    "from ragas import EvaluationDataset, evaluate, SingleTurnSample\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, ResponseRelevancy, LLMContextPrecisionWithoutReference\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "\n",
    "metrics = [\n",
    "    LLMContextPrecisionWithoutReference(llm=evaluator_llm),\n",
    "    LLMContextRecall(llm=evaluator_llm),\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    ResponseRelevancy(llm=evaluator_llm),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  38%|███▊      | 9/24 [00:06<00:08,  1.83it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  42%|████▏     | 10/24 [00:07<00:08,  1.64it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 24/24 [00:17<00:00,  1.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llm_context_precision_without_reference': 0.3250, 'context_recall': 0.1667, 'faithfulness': 0.5661, 'answer_relevancy': 0.2822}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate BASELINE (no rerank)\n",
    "baseline_samples = [\n",
    "    SingleTurnSample(\n",
    "        user_input=r[\"user_input\"],\n",
    "        retrieved_contexts=r[\"retrieved_contexts\"],\n",
    "        response=r[\"response\"],\n",
    "        reference=r[\"reference\"],\n",
    "    )\n",
    "    for r in baseline_results\n",
    "]\n",
    "\n",
    "baseline_dataset = EvaluationDataset(samples=baseline_samples)\n",
    "baseline_eval = evaluate(dataset=baseline_dataset, metrics=metrics)\n",
    "baseline_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 24/24 [00:16<00:00,  1.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llm_context_precision_without_reference': 0.4500, 'context_recall': 0.4167, 'faithfulness': 0.5736, 'answer_relevancy': 0.1350}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate IMPROVED (with rerank)\n",
    "rerank_samples = [\n",
    "    SingleTurnSample(\n",
    "        user_input=r[\"user_input\"],\n",
    "        retrieved_contexts=r[\"retrieved_contexts\"],\n",
    "        response=r[\"response\"],\n",
    "        reference=r[\"reference\"],\n",
    "    )\n",
    "    for r in rerank_results\n",
    "]\n",
    "\n",
    "rerank_dataset = EvaluationDataset(samples=rerank_samples)\n",
    "rerank_eval = evaluate(dataset=rerank_dataset, metrics=metrics)\n",
    "rerank_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results / Reflection\n",
    "\n",
    "| Metric | Hybrid (RRF only) | Hybrid + Rerank | Delta |\n",
    "|--------|-------------------|-----------------|-------|\n",
    "| Context Precision | 0.325 | 0.450 | +0.125 |\n",
    "| Context Recall | 0.167 | 0.417 | +0.250 |\n",
    "| Faithfulness | 0.566 | 0.574 | +0.008 |\n",
    "| Response Relevancy | 0.282 | 0.135 | -0.147 |\n",
    "\n",
    "#### What conclusions can you draw about the performance and effectiveness of your original pipeline?\n",
    "The initial approach (hybrid with RRF) has fairly low context precision (0.325) and context recall (0.167). This indicates that that retrieved images often don't match the user's query well, and we may be missing relevant images. Faithfulness (0.566) is moderate, which is expected since the generated answers are based directly on retrieved descriptions. The low recall suggests that Hybrid + RRF fusion alone isn't surfacing all relevant results.\n",
    "\n",
    "#### How does the performance compare to your original RAG application? \n",
    "Context precision improved by ~38% and context recall more than doubled! This confirms that adding reranking effectively re-orders retrieved results to boost those that are more relevant. The metrics overall are still low, but I believe some of that is due to the small test set and my artificial \"answer generation\" step, since my actual app returns images to the user, not text. I believe this may be the cause of the the small drop response relevancy with reranking as well. Overall, Cohere reranking has improved retrieval quality significantly and I will keep this step in my final application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
